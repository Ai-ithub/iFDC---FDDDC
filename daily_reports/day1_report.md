# نام و نام خانوادگی:امین مقدم
**حوزه فعالیت:** هوش مصنوعی
**تاریخ:** ۱۴۰۳/۰۴/۲۲  
**شماره گزارش:** ۱  

---

##  فاز مطالعاتی – شناخت پروژه و نیازمندی‌ها

در روز اول تمرکز بر مطالعه و درک ساختار پروژه بود. فایل `README.md` پروژه به عنوان **سند SRS** (مشخصات نیازمندی‌های نرم‌افزار) تحلیل شد.

###  خلاصه اهداف پروژه:
- توسعه سیستم هوشمند **FDMS** جهت پایش و پیش‌بینی **آسیب‌سازند** در حین عملیات حفاری
- استفاده از داده‌های لحظه‌ای MWD/LWD و الگوریتم‌های یادگیری ماشین (XGBoost, LSTM, GRU)
- ایجاد داشبورد تعاملی برای **مانیتورینگ، تحلیل و هشدار در لحظه**

###  اجزای کلیدی پروژه:
-اعتبار سنجی داده ها
- ماژول پیش‌بینی ریسک نشت سیال و امولسیون
-  مدل‌های پیش‌بینی آسیب سازند
-  داشبورد فرانت‌اند با Plotly + React
###  ساختار پروژه:
پروژه شامل ماژول‌های جداگانه برای:
- داده‌ها (`dataset/`)
- مدل‌ها (`models/`)
- پردازش اولیه (`preprocces_pipeline.py`)
- داشبورد و API (`frontend/`, `backend/`)

---

##  برنامه کاری فردا – شروع فاز عملیاتی

###  اولویت‌های فردا (روز دوم):
1. بررسی دقیق فایل `preprocces_pipeline.py` برای درک منطق پاک‌سازی و آماده‌سازی داده‌ها
2. اجرای تستی Pipeline روی داده‌های موجود در `dataset/`
3. ساخت فایل CSV خروجی تمیز برای استفاده در مدل‌سازی
4. مستندسازی روند پردازش داده‌ها برای گزارش روز دوم

---

##  پیشنهاد فنی (محتوای پیشنهادی):
برای شروع عملیاتی، تمرکز روی بخش Data Validation هوشمندانه است. فایل `preprocces_pipeline.py` را به صورت خط‌به‌خط بررسی کرده و:
- نوع خطاهایی که چک می‌شوند را دسته‌بندی کنیم (Nulls, Ranges, Units)
- اگر داده مصنوعی یا ناقص بود، آن را پاکسازی یا برچسب‌گذاری کنیم
- خروجی تمیز شده را در `data/clean/` ذخیره کنیم (می‌توان این مسیر را ایجاد کرد)

در صورت نیاز، از ابزارهای زیر استفاده شود:
- برای تحلیل و پاک‌سازی(pandas)
-  برای بررسی توزیع داده‌ها(seaborn ,matplotlib)
- امکان افزودن تست اولیه برای بررسی pipeline در `notebooks/` وجود دارد

---


