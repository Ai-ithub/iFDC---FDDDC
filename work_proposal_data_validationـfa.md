# 🚀 پیشنهاد کاری

## شناسایی و مدیریت مشکل داده‌های تکراری

**تهیه شده توسط:**

- **نام:** شایان طالبیان
- **حوزه:** هوش مصنوعی
- **عنوان پروژه:** iFDC---FDDDC
- **ایمیل:** <Shayantalebianwork@gmail.com>
- **نام شاخه:** Shayan-Talebian
- **تاریخ شروع:** 1404/04/16 (2025-07-07)
- **تاریخ:** 1404/04/24 (2025-07-15)

---

## 🎯 هدف پروژه

هدف اصلی این وظیفه، شناسایی رکوردهای تکراری در لاگ‌های داده حفاری، تحلیل دقیق دلایل بروز آنها و اعمال استراتژی‌های پاک‌سازی مناسب (مانند حذف یا تجمیع) است.  
این فرایند موجب حفظ یکپارچگی داده‌ها، جلوگیری از ایجاد الگوهای گمراه‌کننده و ارتقای کیفیت کلی مجموعه داده‌ها برای آموزش مدل‌های یادگیری ماشین و تحلیل‌های پیش‌بینی می‌شود.

---

## 🔍 دامنه کاری به‌صورت جزئی

### 1. تحلیل اکتشافی تکرارها

- بارگذاری مجموعه داده‌های خام حفاری در قالب **CSV** یا **Parquet**.
- شناسایی:
  - **ردیف‌های کاملاً تکراری:** با استفاده از چک‌های استاندارد DataFrame.
  - **تکرارهای جزئی:** رکوردهایی که ویژگی‌های کلیدی مشابه (مثل `timestamp`، `depth`، `pressure` و ...) دارند ولی در سایر ستون‌ها متفاوتند.
- تهیه خلاصه‌های آماری:
  - شمارش فراوانی رکوردهای تکراری.
  - درصد رکوردهای تکراری نسبت به کل.
  - توزیع آنها در ویژگی‌ها برای درک خوشه‌بندی تکرارها.

### 2. بررسی دلایل اصلی

- مطالعه علل احتمالی تکراری شدن داده‌ها:
  - نویز یا خطای سنسورها که منجر به ثبت مکرر می‌شود.
  - سیستم‌های خودکار که در هنگام تلاش مجدد یا failover، داده‌ها را دوباره ثبت می‌کنند.
  - ادغام تاریخی داده‌ها از منابع مختلف بدون حذف تکرار.

### 3. تدوین استراتژی پاک‌سازی

- تعریف سیاست‌های مشخص برای برخورد با تکرارها:
  - **تکرارهای دقیق:** حذف همه موارد به جز یکی.
  - **تکرارهای جزئی:** تجمیع با میانگین‌گیری، انتخاب آخرین رکورد، یا بر اساس بالاترین شاخص‌های کیفیت داده.
- مستندسازی این سیاست‌ها در یک فایل `config.yaml` برای شفافیت و قابلیت بازتولید.

### 4. پیاده‌سازی پاک‌سازی داده‌ها

- ساخت اسکریپت (یا نوت‌بوک) قوی که:
  - داده‌ها را بارگذاری کند.
  - استراتژی حذف تکرار را اعمال کند.
  - داده پاک‌شده را در `data/cleaned/no_duplicates.csv` ذخیره کند.
- پیاده‌سازی پارامترهای اختیاری مانند:
  - `--method mean` یا `--method last`
  - `--dry-run` برای پیش‌نمایش بدون ذخیره تغییرات.

### 5. گزارش‌گیری و مستندسازی

- تولید یک گزارش متنی (`reports/duplicates_summary.txt`) شامل:
  - تعداد رکوردها قبل و بعد از پاک‌سازی.
  - تعداد تکرارهای کامل حذف شده.
  - استراتژی به‌کاررفته.
  - نکات مربوط به فرضیات و ملاحظات کیفیت داده.
- در صورت نیاز ذخیره نمودارها (PNG/SVG) که الگوهای تکرار را نمایش می‌دهند.

---

## 🛠 ابزارها و تکنولوژی‌های مورد استفاده

| ابزار / کتابخانه     | کاربرد                                |
| -------------------- | ------------------------------------- |
| Python (3.9+)        | زبان اصلی                             |
| Pandas               | بارگذاری و پردازش داده                |
| NumPy                | عملیات عددی و آماری                   |
| Matplotlib / Seaborn | مصورسازی و نمودارهای توزیع            |
| Jupyter Notebook     | تحلیل اکتشافی و مستندسازی تعاملی      |
| Click / argparse     | رابط خط فرمان برای اسکریپت‌ها         |
| rich (اختیاری)       | خروجی‌های زیبا در ترمینال             |
| pytest (اختیاری)     | تست‌های واحد برای اعتبارسنجی پاک‌سازی |

---

## 🗂 فایل‌ها و خروجی‌ها

| فایل/پوشه                            | توضیحات                               |
| ------------------------------------ | ------------------------------------- |
| `data/raw/*.csv or *.parquet`        | فایل‌های داده خام حفاری               |
| `scripts/remove_duplicates.py`       | اسکریپت حذف تکرار                     |
| `notebooks/duplicate_analysis.ipynb` | نوت‌بوک برای تحلیل اکتشافی و نمودارها |
| `config.yaml`                        | پیکربندی سیاست‌های پاک‌سازی           |
| `data/cleaned/no_duplicates.csv`     | مجموعه داده نهایی پاک‌شده             |
| `reports/duplicates_summary.txt`     | گزارش متنی مدیریت تکرارها             |

---

## 💡 پیشنهادات ارتقایی

برای بیشینه‌سازی ارزش و نگه‌داری در بلندمدت:

- ✅ **ماژولار و قابل پیکربندی:**  
  منطق پاک‌سازی با پارامترهای خط فرمان و `config.yaml` قابل تنظیم باشد.

- ✅ **لاگ‌گیری حرفه‌ای:**  
  استفاده از `rich` یا `loguru` برای لاگ‌های رنگی و خوانا که پیگیری رکوردهای حذف/تجمیع‌شده را آسان می‌کند.

- ✅ **پشتیبانی از Dry Run:**  
  امکان مشاهده اقدامات پاک‌سازی بدون ذخیره خروجی (`--dry-run`) برای شفافیت کامل.

- ✅ **هیتمپ و هیستوگرام:**  
  مصورسازی خوشه‌های تکرار بر اساس عمق و زمان که می‌تواند مشکلات عملیاتی را نیز نمایان کند.

- ✅ **تست‌های واحد:**  
  اطمینان از عدم وجود تکرار پس از پاک‌سازی و کارکرد صحیح استراتژی‌های تجمیع.

- ✅ **قابلیت مقیاس‌پذیری در آینده:**  
  طراحی به‌گونه‌ای که بتوان با کمترین تغییر، داده‌های چاه‌های دیگر یا چندچاهی را نیز پردازش کرد.

---

## ✈ خروجی‌های مورد انتظار

🎯 در پایان پروژه، موارد زیر تحویل داده می‌شود:

- 📊 گزارش تحلیلی کامل (`reports/duplicates_summary.txt`) شامل تعداد تکرارها، استراتژی پاک‌سازی و دلایل اصلی.
- 🗂 مجموعه داده پاک‌شده (`data/cleaned/no_duplicates.csv`).
- ⚙ اسکریپت پایتون (`scripts/remove_duplicates.py`) آماده استفاده مجدد با گزینه‌های CLI.
- 📖 نوت‌بوک اکتشافی (`notebooks/duplicate_analysis.ipynb`) همراه با نمودار و توضیحات.
- 📝 یک `config.yaml` نمونه برای قوانین پاک‌سازی.
- (در صورت نیاز) تست‌های واحد که اطمینان می‌دهند تکراری باقی نمانده است.

---

## 💎 ارزش و تأثیر تجاری

- 📈 **ارتقای یکپارچگی داده:**  
  اطمینان از آموزش مدل‌های یادگیری ماشین بر داده‌های باکیفیت و بدون تکرار.

- ⏱ **تسریع چرخه‌های تکرار:**  
  با وجود داده پاک و پایپ‌لاین قابل استفاده مجدد، پردازش چاه‌های بعدی با حداقل تلاش انجام می‌شود.

- 🔬 **بینش‌های عملیاتی عمیق‌تر:**  
  حتی هیتمپ‌های تکرار می‌توانند به تیم‌های حفاری در بهبود کالیبراسیون سنسورها یا پروتکل‌های ثبت داده کمک کنند.

---

## 👨‍💻 آماده شروع

این پیشنهاد برای ارائه قابلیت‌های پاک‌سازی داده قوی، مستند و مقیاس‌پذیر به‌طور خاص برای لاگ‌های حفاری شما طراحی شده است.  
این کار نه تنها کیفیت فوری داده‌های شما را ارتقا می‌دهد، بلکه استانداردی طلایی برای پایپ‌لاین‌های مشابه در چاه‌های آینده ایجاد می‌کند.

---

## 🔎 نکته‌ای درباره یادگیری و توسعه مهارت

مایلم با شفافیت کامل اعلام کنم که در حال حاضر یک دانشمند داده در سطح جونیور هستم.  
درک محکمی از **پایتون** دارم و در کار با **Pandas** برای پردازش و تحلیل اکتشافی داده کاملاً راحت هستم.  
هم‌اکنون به‌طور فعال در حال بهبود مهارت‌های خود در **NumPy** و تکنیک‌های آماری و پاک‌سازی پیشرفته‌تر هستم.

این پروژه کاملاً با مسیر یادگیری من هم‌راستاست.  
ممکن است هنوز در همه جنبه‌ها (به‌ویژه در پاک‌سازی آماری پیچیده یا ساخت پایپ‌لاین‌های تولیدی) تجربه عمیق عملی نداشته باشم، اما کاملاً متعهد به یادگیری و تحقیق درباره بهترین شیوه‌ها برای ارائه نتایج باکیفیت هستم.

اطمینان دارم که با تلاش متمرکز، زمان و یادگیری تدریجی، قادر به تکمیل موفقیت‌آمیز تمام وظایف این پیشنهاد خواهم بود.  
برایم صداقت بسیار ارزشمند است و ترجیح می‌دهم سطح مهارتم را به‌روشنی بیان کنم، چرا که عمیقاً به رشد از طریق چالش‌های واقعی باور دارم.

از شما بابت در نظر گرفتن این موضوع سپاسگزارم و مشتاقم که علاوه بر تحویل این پروژه، تخصص خود را نیز در این مسیر ارتقا دهم.

---

**تهیه شده توسط:**  
**شایان طالبیان**
